# 人工神经网络 期末项目

20337251 伍建霖

## 一、算法原理

### 词嵌入

词嵌入是自然语言处理中一种常用的技术，用于将文本中的单词或短语映射到实数值向量空间中。它是将离散的文本表示转换为连续的向量表示的一种方法。

在传统的文本处理中，单词通常以离散的形式表示，例如使用独热编码将每个单词表示为一个高维稀疏向量，其中只有一个元素为1，其余元素为0。然而，这种表示方式存在一些问题，如维度灾难和无法捕捉词语之间的语义关系。

词嵌入通过将每个单词映射到一个连续的向量空间中的实数向量来解决这些问题。在这个向量空间中，相似的词在几何上更接近，可以通过计算它们之间的距离或相似度来捕捉它们之间的语义关系。这种表示方式能够更好地捕捉单词的上下文信息和语义含义，从而提供更丰富的语义表达。

词嵌入可以通过不同的方法来学习，其中最常用的方法是使用神经网络模型，如Word2Vec、GloVe和BERT等。这些模型会在大规模的文本语料库上进行训练，通过预测单词的上下文或利用语言模型来学习词嵌入表示。学习得到的词嵌入模型可以应用于各种NLP任务，如文本分类、命名实体识别、情感分析等，以提升模型的性能和效果。

### LSTM

LSTM是RNN的一个变体，用于处理序列数据，特别是在自然语言处理和时间序列分析领域。

传统的循环神经网络在处理长序列时存在梯度消失或梯度爆炸的问题，导致难以捕捉长期依赖关系。LSTM通过引入一种称为"门控"机制的方式来解决这个问题，使得网络能够更好地处理长期依赖。

LSTM单元由多个门控单元组成，包括遗忘门、输入门和输出门。每个门控单元都包含一个sigmoid激活函数，用于控制信息的流动。通过这些门控机制，LSTM可以选择性地遗忘或记住输入数据的部分信息，并决定哪些信息应该传递到下一个时间步。

在LSTM中，每个单元有一个隐藏状态和一个细胞状态。隐藏状态类似于传统RNN中的输出，而细胞状态则用于传递和存储长期记忆。通过门控机制，LSTM可以根据输入数据和上一个时间步的隐藏状态来更新细胞状态和隐藏状态。

LSTM在处理序列数据时具有较好的记忆能力和长期依赖建模能力，使得它在许多NLP任务中表现出色。例如，它可以用于文本生成、机器翻译、语音识别、情感分析等任务，也可以作为其他更复杂模型的组成部分，如语言模型和序列到序列模型。

### 注意力机制

注意力机制（Attention Mechanism）是一种用于增强神经网络在处理序列或集合数据时的能力的机制。它允许模型在处理输入序列时集中关注其中的特定部分，从而更好地捕捉关键信息和上下文之间的关系。

在传统的神经网络中，每个输入都会以相同的权重进行处理，无论其在序列中的位置或重要性如何。而注意力机制通过引入可学习的权重，使模型能够动态地对输入序列中的不同部分分配不同的注意力权重。

在使用注意力机制的模型中，一般会有两个主要组件：查询（Query）、键（Key）和值（Value）。查询用于指定模型要关注的部分，键和值分别表示输入序列的信息。通过计算查询和键之间的相似度，可以得到每个键对于查询的注意力权重。然后，通过将值与对应的注意力权重加权求和，得到聚合后的表示，以便模型更关注重要的部分。

注意力机制可以应用于各种神经网络模型中，包括RNN、CNN和自注意力机制。自注意力机制（如Transformer模型中的注意力机制）在自然语言处理领域中得到广泛应用，并在机器翻译、文本摘要、语言建模等任务中取得了显著的性能提升。

总之，注意力机制使得模型能够有选择性地聚焦于输入序列中的不同部分，并通过动态分配权重来捕捉重要的信息，从而提高了模型的表达能力和性能。


## 二、实验代码



## 三、实验结果



## 四、遇到的问题

1. linux环境导致训练卡住
2. 层数增加效果变差
3. 验证集上表现

## 五、参考资料

[从简单RNN到带Attention LSTM机器翻译基本思路 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/61494510)


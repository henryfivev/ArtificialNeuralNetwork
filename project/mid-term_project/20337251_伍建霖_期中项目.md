# 人工神经网络 期中项目

20337251 伍建霖

## 一、神经网络原理

### 卷积层Conv

每次作用于图像的一部分，用于提取图像的局部特征。有以下几种参数：

1. channel：输入输出通道数，rgb为三通道
2. kernel_size：卷积核大小
3. stride：步长，即每次卷积核移动的像素个数
4. padding：填充的像素个数

### 激活函数

激活函数是用来加入非线性因素的，因为线性模型的表达力不够。常用激活函数有以下几种：

1. ReLu：${max(x, 0)}$
2. sigmoid：${\frac {1}{1+e^{-x}}}$
3. tanh：${\frac {e^x-e^{-x}}{e^x+e^{-x}}}$

### 池化层

对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。还具有平移不变性和旋转不变性。往往在卷积层后面使用，通过池化来降低卷积层输出的特征向量，同时改善结果，避免过拟合。有以下几种常用的池化层：

1. 最大池化：取窗口内最大值
2. 平均池化：取窗口内平均值

### 批归一化BN

操作：先求平均值与方差，再标准化即可

作用：做了归一化之后，可以近似的认为训练数据和测试数据满足相同分布(即均值为0，方差为1的标准正态)，这样一来模型的泛化能力会得到提高。其次：如果不做归一化，使用mini-batch梯度下降法训练的时候，每批训练数据的分布不相同，那么网络就要在每次迭代的时候去适应不同的分布，这样会大大降低网络的训练速度。

### 全连接层FC

全连接层一般会把卷积输出的二维特征图转化为一维的一个向量，全连接层的每一个节点都与上一层的每个节点连接，是把前一层的输出特征都综合起来，所以该层的权值参数是最多的。全连接网络的作用就是将最后一层卷积得到的特征flatten成向量，对这个向量做乘法，最终降低其维度，然后输入到softmax层中得到对应的每个类别的得分。但参数量过大，特别是与最有一个卷积层（池化层）相连的全连接层。参数量过大会导致训练速度降低，容易过拟合。

### 随机失活Dropout

在训练阶段按某种概率随即将输入的张量元素随机归零，用于防止网络过拟合。

### 随机梯度下降SGD

简单来说就是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次。在每次更新时用1组样本，用这组样本来近似所有的样本，以此更新参数。虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。

### 数据增强

对数据进行变化，使模型具有泛化能力。就图像数据而言，有以下数据增强操作：

1. 裁剪：中心裁剪，随机裁剪
2. 翻转和旋转：水平翻转，垂直翻转，随机旋转
3. 图像变化：亮度，对比度，饱和度，色相
4. 操作本身：随机顺序，随机执行，

## 二、实验代码

### 搭建网络

这里我使用pytorch的nn.module来搭建神经网络

```python
# 定义卷积神经网络模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Sequential(
            nn.BatchNorm2d(3),
            nn.Conv2d(3, 96, 7, 2, 1,),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.BatchNorm2d(96),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(96, 256, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(256, 384, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.BatchNorm2d(384),
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(384, 512, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.BatchNorm2d(512),
        )
        self.fc1 = nn.Linear(512 * 6*6, 2048)
        self.fc2 = nn.Linear(2048, 2048)
        self.fc3 = nn.Linear(2048, 500)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = x.view(x.size(0), -1)  # 展平多维的卷积图层
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        x = self.dropout(x)
        return x
```



### 数据增强

这里我使用pytorch的tranform，使得在数据预处理阶段就进行数据增强。依次调整了图像的亮度，饱和度，对比度和色相，以及随机水平翻转。

```python
transform = transforms.Compose(
    [
        transforms.ColorJitter(brightness=0.5),
        transforms.ColorJitter(saturation=0.5),
        transforms.ColorJitter(contrast=0.5),
        transforms.ColorJitter(hue=0.5),
        transforms.RandomHorizontalFlip(0.5),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ]
)
```



### 读入数据

这里我使用imagefolder读入数据，用dataloader设置数据集，用tranform设置数据预处理并设置shuffle来打乱数据

```python
train_dataset = ImageFolder(
    "./face_classification_500/train_sample", transform=transform
)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
dev_dataset = ImageFolder("./face_classification_500/dev_sample", transform=transform)
dev_loader = DataLoader(dev_dataset, batch_size=64, shuffle=False)
test_dataset = ImageFolder("./face_classification_500/test_sample", transform=transform)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
```

### 设置参数

```python
net = Net().to(devicee)
criterion = nn.CrossEntropyLoss().to(devicee)
optimizer = optim.SGD(net.parameters(), lr=0.007)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    factor=0.98,
    patience=10,
    verbose=False,
    threshold=0.0001,
    threshold_mode="rel",
    cooldown=0,
    min_lr=0,
    eps=1e-08,
)
train_loss = []
dev_acc = []
test_acc = []
```



### 训练阶段



```python
# 训练模型
for epoch in range(100):  # 多次循环数据集
    net.train()
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # 获取输入数据
        inputs, labels = data
        inputs = inputs.to(devicee)
        labels = labels.to(devicee)
        # print(inputs.shape)
        # print(labels.shape)

        # 梯度清零
        optimizer.zero_grad()

        # 正向传递
        outputs = net(inputs)
        loss = criterion(outputs, labels)

        # 反向传递和优化
        loss.backward()
        optimizer.step()

        # 输出统计信息
        train_loss.append(loss.item())
        running_loss += loss.item()
        if i % 10 == 9:
            # 每10个小批次打印一次统计信息
            scheduler.step(running_loss / 10)
            print("[%d, %5d] loss: %.5f" % (epoch + 1, i + 1, running_loss / 10))
            running_loss = 0.0
```



### 测试阶段



```python
net.eval()
correct = 0
total = 0
with torch.no_grad():
    for data in dev_loader:
        inputs, labels = data
        inputs = inputs.to(devicee)
        labels = labels.to(devicee)
        outputs = net(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        print("Accuracy of the network on the dev images: %d %%" % (100 * correct / total))
        dev_acc.append((100 * correct / total))

        correct = 0
        total = 0
        with torch.no_grad():
            for data in test_loader:
                inputs, labels = data
                inputs = inputs.to(devicee)
                labels = labels.to(devicee)
                outputs = net(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                print("Accuracy of the network on the test images: %d %%" % (100 * correct / total))
                test_acc.append((100 * correct / total))
```



### 数据可视化



```python
plt.plot(train_loss)
plt.title("training loss")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.show()

plt.plot(dev_acc)
plt.title("dev acc")
plt.xlabel("Iterations")
plt.ylabel("dev acc")
plt.show()

plt.plot(test_acc)
plt.title("test acc")
plt.xlabel("Iterations")
plt.ylabel("test acc")
plt.show()
```





## 三、实验结果



## 四、遇到的问题

### 梯度爆炸

https://zhuanlan.zhihu.com/p/32154263

### 梯度先上升后下降



### 梯度不收敛

神经网络不收敛可能有以下原因：忘记对数据进行归一化、忘记检查输出结果、没有对数据进行预处理、没有使用任何的正则化方法、使用了一个太大的 batch size、使用一个错误的学习率、在最后一层使用错误的激活函数、网络包含坏的梯度等

## 五、参考资料

https://zhuanlan.zhihu.com/p/22538465

[卷积神经网络各层基本知识](https://blog.csdn.net/zong596568821xp/article/details/80459968?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-80459968-blog-89343769.235^v35^pc_relevant_increate_t0_download_v2_base&spm=1001.2101.3001.4242.1&utm_relevant_index=1)
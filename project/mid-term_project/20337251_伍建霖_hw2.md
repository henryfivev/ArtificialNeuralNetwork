# 人工神经网络 期中项目

20337251 伍建霖

## 神经网络原理

### 卷积层Conv

用于提取特征

参数：

1. 卷积核
2. 步长
3. 填充

### 激活函数

激活函数是用来加入非线性因素的，因为线性模型的表达力不够

常用激活函数有以下几种：

1. ReLu
2. sigmoid
3. tanh

### 池化层

对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征

往往在卷积层后面使用，通过池化来降低卷积层输出的特征向量，同时改善结果，避免过拟合

有以下几种常用的池化层：

1. 最大池化
2. 平均池化



### 批归一化BN



### 全连接层FC

全连接层一般会把卷积输出的二维特征图转化为一维的一个向量，全连接层的每一个节点都与上一层的每个节点连接，是把前一层的输出特征都综合起来，所以该层的权值参数是最多的。

全连接网络的作用就是将最后一层卷积得到的feature map stretch成向量，对这个向量做乘法，最终降低其维度，然后输入到softmax层中得到对应的每个类别的得分。

参数量过大，特别是与最有一个卷积层（池化层）相连的全连接层。参数量过大会导致训练速度降低，容易过拟合。

### 随机失活Dropout

在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，对于最优化问题，凸问题，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。

### 随机梯度下降SGD

在每次更新时用1个样本，可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，对于最优化问题，凸问题，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。

### 数据增强

对数据进行变化，使模型具有泛化能力。

就图像数据而言，有以下数据增强操作：

1. 裁剪：中心裁剪，随机裁剪
2. 翻转和旋转：水平翻转，垂直翻转，随机旋转
3. 图像变化：亮度，对比度，饱和度，色相
4. 操作本身：随机顺序，随机执行，

## 遇到的问题

### 梯度爆炸

https://zhuanlan.zhihu.com/p/32154263

### 梯度先上升后下降



### 梯度不收敛




## 参考资料

https://zhuanlan.zhihu.com/p/22538465

https://blog.csdn.net/zong596568821xp/article/details/80459968?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-80459968-blog-89343769.235^v35^pc_relevant_increate_t0_download_v2_base&spm=1001.2101.3001.4242.1&utm_relevant_index=1